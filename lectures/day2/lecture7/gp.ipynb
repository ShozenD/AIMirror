{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShozenD/AIMirror/blob/master/lectures/day2/lecture7/gp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/resources/logos/imperial.png\" width=\"250\" vspace=\"8\"/>\n",
        "<img src=\"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/resources/logos/mlgh.png\" width=\"220\" hspace=\"50\" vspace=\"5\"/>\n",
        "<img src=\"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/resources/logos/ammi.png\" width=\"190\"/>\n",
        "\n",
        "<font size=\"6\">Modern Statistics and Machine Learning\n",
        "for Population Health in Africa </font>\n",
        "\n",
        "<font size=\"4\">24th - 28th March 2025</font>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "2KPp0HhevBwm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDKRpPW8NO3y"
      },
      "source": [
        "# Gaussian Process Regression with Stan\n",
        "\n",
        "## Objectives\n",
        "In this tutorial, you will learn about how to implement a Gaussian Process (GP) in Stan.\n",
        "\n",
        "By the end of this tutorial,\n",
        "1. You will have a better understanding of implementing custom functions in Stan;\n",
        "2. You will improve your ability to translate mathematics into Stan code;\n",
        "3. You will have a better understanding of GPs and how to implement them in probabilistic programming languages.\n",
        "\n",
        "## Flow of the tutorial\n",
        "1. Brief description of the dataset\n",
        "2. Review of Gaussian Processes\n",
        "3. How to implement a GP in Stan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install CmdStanPy for Google Colab\n",
        "!curl -O \"https://raw.githubusercontent.com/MLGlobalHealth/StatML4PopHealth/main/practicals/resources/scripts/utilities.py\"\n",
        "from utilities import custom_install_cmdstan, test_cmdstan_installation\n",
        "custom_install_cmdstan()"
      ],
      "metadata": {
        "id": "_WxdvkcEvF1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F6XSMeGukcJ"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from cmdstanpy import CmdStanModel\n",
        "import arviz as az\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Aesthetics\n",
        "sns.set_theme(style='whitegrid')\n",
        "plt.rc('font', size=9)\n",
        "plt.rc('axes', titlesize=10)\n",
        "plt.rc('axes', labelsize=9)\n",
        "plt.rc('xtick', labelsize=9)\n",
        "plt.rc('ytick', labelsize=9)\n",
        "plt.rc('legend', fontsize=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFenMVwjJKnm"
      },
      "source": [
        "## The dataset\n",
        "For this tutorial, we will use the `nile` dataset available via the `statsmodels` library. The dataset consists of annual flow measurements of the Nile River at Aswan from 1871 to 1970."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_eS_GwiukcK"
      },
      "outputs": [],
      "source": [
        "from statsmodels.datasets import nile\n",
        "nile = nile.load_pandas().data\n",
        "\n",
        "# Plot the data\n",
        "fig, ax = plt.subplots()\n",
        "nile.plot(x='year', y='volume', ax=ax)\n",
        "ax.set_title('Nile River Volume')\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('Volume (10^8 m^3)')\n",
        "ax.set_xlim(1871, 1970)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mST2_a7lJdwo"
      },
      "source": [
        "### Data preprocessing\n",
        "To make things easier for the model, we will normalise the year data to be between 0 and 1 and standardise the flow data to have a mean of 0 and a standard deviation of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIBtO01YJRMc"
      },
      "outputs": [],
      "source": [
        "volume = nile['volume'].values\n",
        "year = nile['year'].values\n",
        "\n",
        "# Standardise year\n",
        "year_mean = year.mean()\n",
        "year_std = year.std()\n",
        "x = (year - year_mean) / year_std\n",
        "\n",
        "# Standardise volume\n",
        "volume_mean = volume.mean()\n",
        "volume_std = volume.std()\n",
        "y = (volume - volume_mean) / volume_std\n",
        "\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgtv3E1WTX9e"
      },
      "source": [
        "## Review of Gaussian Process Regression\n",
        "\n",
        "Let $\\mathbf{y} = (y_1,\\ldots,y_n)^\\top$ be a vector of outcomes, in this case flow volume. Let $\\mathbf{x} = (x_1,\\ldots,x_n)^\\top$ be a vector of inputs, in this case year. We will model the data as\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbf{y} &= \\alpha + f(\\mathbf{x}) + \\boldsymbol{\\varepsilon}, \\\\\n",
        "f(\\mathbf{x}) &\\sim GP(m(\\mathbf{x}), k(\\mathbf{x},\\mathbf{x}')), \\\\\n",
        "\\boldsymbol{\\varepsilon} &\\sim N(0, \\sigma_{\\varepsilon}^2\\mathbf{I}_n),\n",
        "\\end{align*}\n",
        "$$\n",
        "where $m(\\mathbf{x})$ is the mean function, $k(\\mathbf{x},\\mathbf{x}')$ is the covariance function, and $\\sigma_{\\varepsilon}^2$ is the noise variance. For simplicity, we will assume that $m(\\mathbf{x}) = 0$\n",
        "$$\n",
        "f(\\mathbf{x}) \\sim GP(0, k(\\mathbf{x},\\mathbf{x}')).\n",
        "$$\n",
        "\n",
        "Recall that for finite data, the zero mean GP prior is a multivariate Gaussian distribution with mean vector $\\mathbf{0}$ and covariance matrix $\\mathbf{K}$, where $K_{ij} = k(x_i,x_j)$.\n",
        "$$\n",
        "\\mathbf{K} = \\begin{pmatrix}\n",
        "k(x_1,x_1) & \\cdots & k(x_1,x_n) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "k(x_n,x_1) & \\cdots & k(x_n,x_n)\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "The function $k$ is referred to as a covariance function or kernel function. Common choices for the kernel function include the squared exponential kernel, the Matern kernel, and the periodic kernel. As we shall see later, the choice of kernel characterises the smoothness of the GP. If, for example, we chose the squared exponential kernel, then $k$ has the form\n",
        "$$\n",
        "k(x,x') = \\sigma^2\\exp\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right),\n",
        "$$\n",
        "where $\\sigma^2$ is the marginal variance and $\\ell$ is the lengthscale.\n",
        "\n",
        "To sample from a GP, we first compute the covariance matrix $\\mathbf{K}$ for a given set of inputs $\\mathbf{x}$, kernel $k$, and hyperparameters. We then perform the Cholesky decomposition of $\\mathbf{K}$ such that\n",
        "$$\n",
        "\\mathbf{K} = \\mathbf{L}\\mathbf{L}^\\top,\n",
        "$$\n",
        "where $\\mathbf{L}$ is a lower triangular matrix. We then sample an auxiliary vector $\\mathbf{z}$ from a standard multivariate normal distribution, i.e., $\\mathbf{z} \\sim N(\\mathbf{0},\\mathbf{I}_n)$. Finally, we compute the GP sample as\n",
        "$$\n",
        "\\mathbf{f} = \\mathbf{L}\\mathbf{z}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZKRpua9J0Uh"
      },
      "source": [
        "## Implementing a GP in Stan\n",
        "\n",
        "### The GP prior\n",
        "To keep the code in the other blocks clean, we will define the GP prior in the `functions` block. We will write a custom function named `gp_se` that takes in the input points `x`, the marginal variance `alpha`, and the lengthscale `ell`, and a vector of auxiliary variables `z`. The function will return the GP sample `f`. The code will look something like this:\n",
        "\n",
        "```stan\n",
        "functions {\n",
        "\tvector gp_se(vector x, real sigma, real ell, vector z) {\n",
        "\t\tint n = rows(x);\n",
        "\t\tmatrix[n,n] K;\n",
        "\t\tvector[n] f;\n",
        "\t\tmatrix[n,n] L;\n",
        "\n",
        "\t\t// 1. Compute the covariance matrix (don't forget to add the nuggget term)\n",
        "\n",
        "\t\t// 2. Perform the Cholesky decomposition\n",
        "\n",
        "\t\t// 3. Compute the GP sample\n",
        "\n",
        "\t\treturn f;\n",
        "\t}\n",
        "}\n",
        "```\n",
        "\n",
        "Your task is to fill in the missing parts of the code. Refer to the [Stan Functions Reference](https://mc-stan.org/docs/functions-reference/index.html) on how to compute the covariance matrix, how to construct an identity matrix (for the nugget term), and on how to perform the Cholesky decomposition.\n",
        "\n",
        "### The data block\n",
        "In the data block, we will declare the number of data points `N`, input points `x` and the outcome `y`. Do you remember how to do this?\n",
        "\n",
        "### The parameters block\n",
        "In the parameters block, the hyperparameters of the GP, namely the marginal variance $\\sigma$ and the lengthscale $\\ell$. We also need to declare the intercept $\\alpha$, auxiliary variables $\\mathbf{z}$ and the noise variance $\\sigma_\\varepsilon$.\n",
        "\n",
        "### Transformed parameters block\n",
        "In the transformed parameters block, we will compute the GP sample `f` using the custom function `gp_se` that we defined earlier.\n",
        "\n",
        "### The model block\n",
        "In the model block, we specify the likelihood and the priors for each parameter and the auxiliary random variables. We will assume a normal likelihood for the data and inverse-gamma priors for the hyperparameters. We will also assume an inverse-gamma prior for the noise variance.\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\sigma &\\sim \\text{inv-Gamma}(1,5), \\\\\n",
        "\\ell &\\sim \\text{inv-Gamma}(1,5), \\\\\n",
        "\\sigma_{\\varepsilon} &\\sim \\text{inv-Gamma}(1,5) \\\\\n",
        "z_i &\\sim N(0, 1).\n",
        "\\end{align*}\n",
        "$$\n",
        "In case you are wondering, below is a plot of the density of the inverse-gamma distribution with shape parameter 1 and scale parameter 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVFM3MaWukcL"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Parameters for the inverse-gamma distribution\n",
        "alpha = 1  # shape parameter\n",
        "beta = 5   # scale parameter\n",
        "\n",
        "# Generate x values\n",
        "x_vals = np.linspace(0.1, 20, 1000)  # avoiding x=0 as PDF is undefined there\n",
        "\n",
        "# Calculate PDF\n",
        "pdf = stats.invgamma.pdf(x_vals, alpha, scale=beta)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_vals, pdf, 'b-', lw=2)\n",
        "plt.title('Inverse-Gamma Distribution (α=1, β=5)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.grid(True)\n",
        "plt.xlim(0, 20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFIZeQCukcM"
      },
      "source": [
        "### Generated quantities block\n",
        "Finally, in the generated quantities block, we will compute the log likelihood for each data point (for model comparison) and the predicted values for the input points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Qug3mZKfGd"
      },
      "source": [
        "### Compiling and running the Stan program\n",
        "Recall that to compile the Stan program, we use the `CmdStanModel` class from the `cmdstanpy` library. You will need to specify the path to your `.stan` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDfOaxIRukcM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After compiling the model, you need to prepare the data as a dictionary and pass it to the `CmdStanModel` object's `sample` method."
      ],
      "metadata": {
        "id": "1cXVdya-xUew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxCFgYUSukcM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `sample` method to execute the inference algorithm. Run 4 chains with 500 warmup iterations and 1000 sampling iterations. Set the `adapt_delta` argument to 0.95 (for more stable sampling). Set a seed for replicability."
      ],
      "metadata": {
        "id": "_8ynxNevxYzT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5PHSfK2ukcN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# ==========\n",
        "# Your code here\n",
        "# ==========\n",
        "\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "print(f\"Runtime of the Stan model: {runtime} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LplHs34GukcN"
      },
      "source": [
        "How long did it take the inference algorithm to run?\n",
        "\n",
        "### Posthoc model diagnostics\n",
        "We should always check for issues in the sampling process and whether the MCMC chains have converged. We can use the `CmdStanMCMC` object's `diagnose` method to check for divergences, treedepth issues, and other potential problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBeTwVFLukcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `CmdStanMCMC` object's `summary` method to get a summary of the posterior samples. Check if there are any parameters where $\\hat{R} > 1.1$ and where the `ESS_bulk` is less than 1000."
      ],
      "metadata": {
        "id": "boWEDTO8x63y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xptZbe-nukcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUqWsegZRCJT"
      },
      "source": [
        "### Posterior analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `from_cmdstanpy` function in Arviz to convert the fit object into an inference data object."
      ],
      "metadata": {
        "id": "xODA381IyNgL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEBKG7_9ukcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a dictionary called `custom_summary_fns` that has 3 elements:\n",
        "1. \"median\": A lambda function that calculates the median of a given sequence (50th percentile)\n",
        "2. \"q2.5\": A lambda function that calculates the 2.5% percentile of a given sequence\n",
        "3. \"q97.5\": A lambda function that calculates the 97.5% percentile of a given sequence\n",
        "\n",
        "Pass this dictionary to the `stat_funcs` argument of the `summary` method to obtain the desired summary statistics for `mu`, the expected flow rate."
      ],
      "metadata": {
        "id": "qixaSU22yXpG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR8H4cctukcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the posterior distribution of `mu` agains the real data"
      ],
      "metadata": {
        "id": "fhYuOE7ezGIR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQIC97WnukcO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woql9aegukcO"
      },
      "source": [
        "### Posterior predictive checks\n",
        "In a similar fashion to `mu`, calculate the median and the 95% credible intervals for the posterior predictive distribution. Plot this against the real data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxmzJ-q-Mx6j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the proportion of data points within the 95% posterior credible interval."
      ],
      "metadata": {
        "id": "gcndEfD0zca0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwALq1XDukcO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}